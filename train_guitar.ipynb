{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "# from matplotlib import pyplot as plt\n",
    "import time\n",
    "import os\n",
    "# import Levenshtein as L\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#from torchnlp.nn import WeightDropLSTM\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5463\n"
     ]
    }
   ],
   "source": [
    "# load all that we need\n",
    "dataset = np.load('./midis_array_guitar_new.npy')\n",
    "chord_vocab = np.load('./chord_vocab.npy')\n",
    "\n",
    "split_ratio = 0.95\n",
    "train_dataset = []\n",
    "val_dataset = []\n",
    "for song in dataset:\n",
    "    train_split = int(split_ratio * len(song))\n",
    "    train_dataset.append(song[:train_split])\n",
    "    val_dataset.append(song[train_split:])\n",
    "train_dataset = np.array(train_dataset)\n",
    "val_dataset = np.array(val_dataset)\n",
    "\n",
    "print (len(chord_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MusicDataLoader(DataLoader):\n",
    "    def __init__(self, dataset, batch_size, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.p = 0.95\n",
    "        self.seq_len = 70\n",
    "        self.std = 5\n",
    "        \n",
    "    def sample_seq_len_(self):\n",
    "        rand_p = np.random.random_sample()\n",
    "        if rand_p < self.p:\n",
    "            seq_mean = self.seq_len\n",
    "        else:\n",
    "            seq_mean = self.seq_len // 2\n",
    "        return int(np.random.normal(seq_mean, self.std))\n",
    "\n",
    "    def __iter__(self):\n",
    "        if self.shuffle:\n",
    "            rand_idx = np.random.permutation(len(self.dataset))\n",
    "        else:\n",
    "            rand_idx = np.arange(len(self.dataset))\n",
    "        concate_dataset = torch.from_numpy(np.hstack(self.dataset[rand_idx]))\n",
    "        num_iter = len(concate_dataset) // self.batch_size\n",
    "        concate_dataset = concate_dataset[:num_iter*self.batch_size].view(self.batch_size, -1)\n",
    "        concate_dataset.transpose_(0,1)\n",
    "        index = 0\n",
    "        while index < len(concate_dataset):\n",
    "            seq_len = self.sample_seq_len_();\n",
    "            if index + seq_len > len(concate_dataset):\n",
    "                break\n",
    "            yield concate_dataset[index:index+seq_len-1], concate_dataset[index+1:index+seq_len]\n",
    "            index += seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "\n",
    "class MusicModel(nn.Module):\n",
    "\n",
    "    def __init__(self, note_size, embed_size, nlayers):\n",
    "        super(MusicModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(note_size, embed_size)\n",
    "        self.rnn = nn.LSTM(input_size=embed_size, hidden_size=embed_size, num_layers=nlayers, dropout=0.5)\n",
    "        self.linear = nn.Linear(embed_size, note_size)\n",
    "        self.linear.weight = self.embedding.weight\n",
    "        \n",
    "        self.init_weight()\n",
    "        \n",
    "    def init_weight(self):\n",
    "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
    "\n",
    "    def forward(self, seq_batch): # L x B\n",
    "        seq_batch = self.embedding(seq_batch) # L x B x E\n",
    "        seq_batch, hidden = self.rnn(seq_batch) # L x B x H\n",
    "        seq_batch = self.linear(seq_batch)\n",
    "        return seq_batch, hidden\n",
    "    \n",
    "    def generate(self, seq, n_notes):\n",
    "        generated_notes = []\n",
    "        embed = self.embedding(seq).unsqueeze(1) # L x 1 x E\n",
    "        output_lstm, hidden = self.rnn(embed) # L x 1 x H\n",
    "        output = output_lstm[-1] # 1 x H\n",
    "        logits = self.linear(output) # 1 x V\n",
    "        scores = F.gumbel_softmax(logits)\n",
    "        _,current_note = torch.max(scores,dim=1) # 1 x 1\n",
    "        generated_notes.append(current_note)\n",
    "        if n_notes > 1:\n",
    "            for i in range(n_notes-1):\n",
    "                embed = self.embedding(current_note).unsqueeze(0) # 1 x 1 x E\n",
    "                output_lstm, hidden = self.rnn(embed, hidden) # 1 x 1 x H\n",
    "                output = output_lstm[0] # 1 x H\n",
    "                logits = self.linear(output) # V\n",
    "                scores = F.gumbel_softmax(logits)\n",
    "                _,current_note = torch.max(scores,dim=1) # 1\n",
    "                generated_notes.append(current_note)\n",
    "        return torch.cat(generated_notes,dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model trainer\n",
    "class MusicModelTrainer:\n",
    "    def __init__(self, model, train_loader, val_loader, max_epochs=1, run_id='exp'):\n",
    "        self.model = model.to(DEVICE)\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "        self.epochs = 0\n",
    "        self.max_epochs = max_epochs\n",
    "        self.run_id = run_id\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-3, weight_decay=1.2e-6)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train() # set to training mode\n",
    "        epoch_loss = 0\n",
    "        num_batches = 0\n",
    "        n_notes = 0\n",
    "        for inputs, targets in self.train_loader:\n",
    "            num_batches += 1\n",
    "            batch_loss, n_note = self.train_batch(inputs, targets)\n",
    "            epoch_loss += batch_loss\n",
    "            n_notes += n_note\n",
    "            if (num_batches % 100 == 0):\n",
    "                print ('[TRAIN]  Iter [%d]   Loss: %.4f'\n",
    "                          % (num_batches, batch_loss / n_note))\n",
    "        epoch_loss = epoch_loss / n_notes\n",
    "        self.epochs += 1\n",
    "        print('[TRAIN]  Epoch [%d/%d]   Loss: %.4f'\n",
    "                      % (self.epochs, self.max_epochs, epoch_loss))\n",
    "        self.train_losses.append(epoch_loss)\n",
    "\n",
    "    def train_batch(self, inputs, targets):\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        targets = targets.to(DEVICE)\n",
    "        output, hidden = self.model(inputs)\n",
    "        loss = self.criterion(output.view(-1, output.size(2)), targets.contiguous().view(-1))\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item(), output.size(0) * output.size(1)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        epoch_loss = 0\n",
    "        n_notes = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in self.val_loader:\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                targets = targets.to(DEVICE)\n",
    "                output, hidden = self.model(inputs)\n",
    "                loss = self.criterion(output.view(-1, output.size(2)), targets.contiguous().view(-1))\n",
    "                epoch_loss += loss.item()\n",
    "                n_notes += output.size(0) * output.size(1)\n",
    "            epoch_loss = epoch_loss / n_notes\n",
    "            print('[VAL] Val Loss: %.4f' % epoch_loss)\n",
    "            self.val_losses.append(epoch_loss)\n",
    "    \n",
    "    def save(self):\n",
    "        model_path = os.path.join('./experiments', self.run_id, 'model-{}.pt'.format(self.epochs))\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "    \n",
    "    def load(self, model_path):\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "        print (\"loaded model\")\n",
    "    \n",
    "    def generate(self, seed, n_notes):\n",
    "        self.model.eval()\n",
    "        seq = np.array(seed.split(), dtype=int)\n",
    "        seq = torch.from_numpy(seq).to(DEVICE)\n",
    "        output = model.generate(seq, n_notes)\n",
    "        return output.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "\n",
    "NUM_EPOCHS = 20\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving models, predictions, and generated words to ./experiments/guitar_3layer\n"
     ]
    }
   ],
   "source": [
    "run_id = \"guitar_3layer\"\n",
    "if not os.path.exists('./experiments'):\n",
    "    os.mkdir('./experiments')\n",
    "if not os.path.exists('./experiments/%s' % run_id):\n",
    "    os.mkdir('./experiments/%s' % run_id)\n",
    "print(\"Saving models, predictions, and generated words to ./experiments/%s\" % run_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MusicModel(\n",
      "  (embedding): Embedding(5464, 512)\n",
      "  (rnn): LSTM(512, 512, num_layers=3, dropout=0.5)\n",
      "  (linear): Linear(in_features=512, out_features=5464, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = MusicModel(len(chord_vocab)+1, 512, 3)\n",
    "train_loader = MusicDataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "val_loader = MusicDataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "trainer = MusicModelTrainer(model=model, train_loader=train_loader, val_loader=val_loader,\n",
    "                            max_epochs=NUM_EPOCHS, run_id=run_id)\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAIN]  Iter [100]   Loss: 4.0143\n",
      "[TRAIN]  Iter [200]   Loss: 4.0942\n",
      "[TRAIN]  Iter [300]   Loss: 3.5999\n",
      "[TRAIN]  Iter [400]   Loss: 2.8067\n",
      "[TRAIN]  Iter [500]   Loss: 2.7401\n",
      "[TRAIN]  Epoch [1/20]   Loss: 3.5241\n",
      "[VAL] Val Loss: 2.7756\n",
      "[TRAIN]  Iter [100]   Loss: 2.2352\n",
      "[TRAIN]  Iter [200]   Loss: 1.9899\n",
      "[TRAIN]  Iter [300]   Loss: 1.8114\n",
      "[TRAIN]  Iter [400]   Loss: 1.8598\n",
      "[TRAIN]  Iter [500]   Loss: 2.0215\n",
      "[TRAIN]  Epoch [2/20]   Loss: 1.9984\n",
      "[VAL] Val Loss: 2.0590\n",
      "[TRAIN]  Iter [100]   Loss: 1.4650\n",
      "[TRAIN]  Iter [200]   Loss: 1.6468\n",
      "[TRAIN]  Iter [300]   Loss: 1.7959\n",
      "[TRAIN]  Iter [400]   Loss: 1.4372\n",
      "[TRAIN]  Iter [500]   Loss: 1.3686\n",
      "[TRAIN]  Epoch [3/20]   Loss: 1.5846\n",
      "[VAL] Val Loss: 1.7966\n",
      "[TRAIN]  Iter [100]   Loss: 1.1002\n",
      "[TRAIN]  Iter [200]   Loss: 1.3995\n",
      "[TRAIN]  Iter [300]   Loss: 1.6051\n",
      "[TRAIN]  Iter [400]   Loss: 1.3336\n",
      "[TRAIN]  Iter [500]   Loss: 1.6594\n",
      "[TRAIN]  Epoch [4/20]   Loss: 1.3875\n",
      "[VAL] Val Loss: 1.6760\n",
      "[TRAIN]  Iter [100]   Loss: 1.2816\n",
      "[TRAIN]  Iter [200]   Loss: 1.3363\n",
      "[TRAIN]  Iter [300]   Loss: 1.3361\n",
      "[TRAIN]  Iter [400]   Loss: 1.3296\n",
      "[TRAIN]  Iter [500]   Loss: 1.2639\n",
      "[TRAIN]  Epoch [5/20]   Loss: 1.2623\n",
      "[VAL] Val Loss: 1.5670\n",
      "[TRAIN]  Iter [100]   Loss: 1.2880\n",
      "[TRAIN]  Iter [200]   Loss: 1.1594\n",
      "[TRAIN]  Iter [300]   Loss: 1.0781\n",
      "[TRAIN]  Iter [400]   Loss: 1.2500\n",
      "[TRAIN]  Iter [500]   Loss: 1.1524\n",
      "[TRAIN]  Epoch [6/20]   Loss: 1.1670\n",
      "[VAL] Val Loss: 1.5024\n",
      "[TRAIN]  Iter [100]   Loss: 0.8802\n",
      "[TRAIN]  Iter [200]   Loss: 1.1609\n",
      "[TRAIN]  Iter [300]   Loss: 1.1863\n",
      "[TRAIN]  Iter [400]   Loss: 1.0180\n",
      "[TRAIN]  Iter [500]   Loss: 1.2402\n",
      "[TRAIN]  Epoch [7/20]   Loss: 1.0930\n",
      "[VAL] Val Loss: 1.4697\n",
      "[TRAIN]  Iter [100]   Loss: 0.9400\n",
      "[TRAIN]  Iter [200]   Loss: 1.0603\n",
      "[TRAIN]  Iter [300]   Loss: 1.3953\n",
      "[TRAIN]  Iter [400]   Loss: 0.8788\n",
      "[TRAIN]  Iter [500]   Loss: 0.9392\n",
      "[TRAIN]  Epoch [8/20]   Loss: 1.0352\n",
      "[VAL] Val Loss: 1.4241\n",
      "[TRAIN]  Iter [100]   Loss: 0.9373\n",
      "[TRAIN]  Iter [200]   Loss: 0.9924\n",
      "[TRAIN]  Iter [300]   Loss: 1.0475\n",
      "[TRAIN]  Iter [400]   Loss: 0.7868\n",
      "[TRAIN]  Iter [500]   Loss: 1.0747\n",
      "[TRAIN]  Epoch [9/20]   Loss: 0.9819\n",
      "[VAL] Val Loss: 1.4019\n",
      "[TRAIN]  Iter [100]   Loss: 1.1317\n",
      "[TRAIN]  Iter [200]   Loss: 0.8752\n",
      "[TRAIN]  Iter [300]   Loss: 0.9580\n",
      "[TRAIN]  Iter [400]   Loss: 0.9764\n",
      "[TRAIN]  Iter [500]   Loss: 0.9480\n",
      "[TRAIN]  Epoch [10/20]   Loss: 0.9356\n",
      "[VAL] Val Loss: 1.3721\n",
      "[TRAIN]  Iter [100]   Loss: 0.8204\n",
      "[TRAIN]  Iter [200]   Loss: 0.9627\n",
      "[TRAIN]  Iter [300]   Loss: 1.0589\n",
      "[TRAIN]  Iter [400]   Loss: 0.9596\n",
      "[TRAIN]  Iter [500]   Loss: 1.0097\n",
      "[TRAIN]  Epoch [11/20]   Loss: 0.9014\n",
      "[VAL] Val Loss: 1.3503\n",
      "[TRAIN]  Iter [100]   Loss: 0.7860\n",
      "[TRAIN]  Iter [200]   Loss: 0.8483\n",
      "[TRAIN]  Iter [300]   Loss: 1.0683\n",
      "[TRAIN]  Iter [400]   Loss: 0.8025\n",
      "[TRAIN]  Iter [500]   Loss: 0.9966\n",
      "[TRAIN]  Epoch [12/20]   Loss: 0.8625\n",
      "[VAL] Val Loss: 1.3321\n",
      "[TRAIN]  Iter [100]   Loss: 0.7894\n",
      "[TRAIN]  Iter [200]   Loss: 0.8157\n",
      "[TRAIN]  Iter [300]   Loss: 0.8220\n",
      "[TRAIN]  Iter [400]   Loss: 0.8740\n",
      "[TRAIN]  Iter [500]   Loss: 0.7170\n",
      "[TRAIN]  Epoch [13/20]   Loss: 0.8297\n",
      "[VAL] Val Loss: 1.3237\n",
      "[TRAIN]  Iter [100]   Loss: 0.8911\n",
      "[TRAIN]  Iter [200]   Loss: 0.7409\n",
      "[TRAIN]  Iter [300]   Loss: 0.5942\n",
      "[TRAIN]  Iter [400]   Loss: 0.7286\n",
      "[TRAIN]  Iter [500]   Loss: 0.8946\n",
      "[TRAIN]  Epoch [14/20]   Loss: 0.8011\n",
      "[VAL] Val Loss: 1.3097\n",
      "[TRAIN]  Iter [100]   Loss: 0.6962\n",
      "[TRAIN]  Iter [200]   Loss: 0.8280\n",
      "[TRAIN]  Iter [300]   Loss: 0.7004\n",
      "[TRAIN]  Iter [400]   Loss: 0.8169\n",
      "[TRAIN]  Iter [500]   Loss: 0.8203\n",
      "[TRAIN]  Epoch [15/20]   Loss: 0.7756\n",
      "[VAL] Val Loss: 1.2951\n",
      "[TRAIN]  Iter [100]   Loss: 0.8048\n",
      "[TRAIN]  Iter [200]   Loss: 0.6826\n",
      "[TRAIN]  Iter [300]   Loss: 0.9775\n",
      "[TRAIN]  Iter [400]   Loss: 0.8279\n",
      "[TRAIN]  Iter [500]   Loss: 0.8353\n",
      "[TRAIN]  Epoch [16/20]   Loss: 0.7542\n",
      "[VAL] Val Loss: 1.2883\n",
      "[TRAIN]  Iter [100]   Loss: 0.8475\n",
      "[TRAIN]  Iter [200]   Loss: 0.6589\n",
      "[TRAIN]  Iter [300]   Loss: 0.8638\n",
      "[TRAIN]  Iter [400]   Loss: 0.7078\n",
      "[TRAIN]  Iter [500]   Loss: 0.7193\n",
      "[TRAIN]  Epoch [17/20]   Loss: 0.7319\n",
      "[VAL] Val Loss: 1.2836\n",
      "[TRAIN]  Iter [100]   Loss: 0.6115\n",
      "[TRAIN]  Iter [200]   Loss: 0.6767\n",
      "[TRAIN]  Iter [300]   Loss: 0.7358\n",
      "[TRAIN]  Iter [400]   Loss: 0.7500\n",
      "[TRAIN]  Iter [500]   Loss: 0.7924\n",
      "[TRAIN]  Epoch [18/20]   Loss: 0.7156\n",
      "[VAL] Val Loss: 1.2744\n",
      "[TRAIN]  Iter [100]   Loss: 0.9189\n",
      "[TRAIN]  Iter [200]   Loss: 0.6483\n",
      "[TRAIN]  Iter [300]   Loss: 0.8987\n",
      "[TRAIN]  Iter [400]   Loss: 0.7379\n",
      "[TRAIN]  Iter [500]   Loss: 0.6612\n",
      "[TRAIN]  Epoch [19/20]   Loss: 0.6961\n",
      "[VAL] Val Loss: 1.2756\n",
      "[TRAIN]  Iter [100]   Loss: 0.7252\n",
      "[TRAIN]  Iter [200]   Loss: 0.5809\n",
      "[TRAIN]  Iter [300]   Loss: 0.7548\n",
      "[TRAIN]  Iter [400]   Loss: 0.7069\n",
      "[TRAIN]  Iter [500]   Loss: 0.7520\n",
      "[TRAIN]  Epoch [20/20]   Loss: 0.6806\n",
      "[VAL] Val Loss: 1.2654\n"
     ]
    }
   ],
   "source": [
    "best_nll = 1e30  # set to super large value at first\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    trainer.train()\n",
    "    nll = trainer.evaluate()\n",
    "trainer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  17   16   16   16   16   17   17  109  109  109  109  106 5463  106\n",
      " 5463  106 5463  106 5463  106 5463  106 5463  106   43    0   16 5463\n",
      "   16 5463   16 5463   16   12 5463   12   27    0    0   12   16    0\n",
      "   16 5463   16 1287  865  865  865  865    0    0   14   16 5463   16\n",
      "   16   16   16 5463   16  308 5463  308 5463  308   16   16   16  865\n",
      "  865  865  865   36    0    0    0   14   16    0   16    0 1254    0\n",
      "    0   16   16    0   16    0  353    0    0 1275 5463 1275    0 1275\n",
      " 5463 1275 5463 1275 1275   14 5463   14   30   28   16 5463   16  587\n",
      " 5463  587 5463  587 5463  587 5463  587    0    0  105   38   16   16\n",
      " 5463   16   16   16   16  208  208  208  208 5463  208  208 1235  105\n",
      "  105  105  105  105  105  105  105  105  105    0    0    0    0    0\n",
      "    0    0    0  105  105 5463  105  105    0    0  105 5463  105  334\n",
      "  105    0  105  105 5463  105  267 5463  267 5463  267 5463  267 5463\n",
      "  267 5463  267 5463  267 5463  267 5463  267  105  105  105 5463  105\n",
      " 5463  105   38   36 5463   36    8  105  105 5463  105 5463  105  198\n",
      "  198   36   36 5463   36   36   36   36 5463   36   36   61   61 5463\n",
      "   61 5463   61 5463   61 5463   61   30   28   30   28   30   28 5463\n",
      "   28   38   30   38   16   30   38   16 5463   16   61   61   61   88\n",
      "   88   88   88   88   38 5463   38   43 5463   43   88   88   88   88\n",
      "   38 5463   38   43 5463   43   88   88   88   88   38 5463   38    8\n",
      "    8   38    8    8   30   30    8    8   30    8   38 5463   38    8\n",
      "  105  127  127 5463  127 5463  127   38   36 5463   36    8  105  105\n",
      "   88 5463   88   38   36 5463   36    8  105  105   88 5463   88   38\n",
      "   36 5463   36    8  105  105   88 5463   88   38  267 5463  267 5463\n",
      "  267  797 2438   38 5463   38    8   14  105  105 5463  105   38  338\n",
      "    8  180   42  338   42 5463   42   42 1254 1254 1254 1254 1254 1254\n",
      " 1254 5463 1254 1254  230   38   10  334   38   38   38   38   38   38\n",
      " 3312 3312 3312 3312   36   36 5463   36   36   38   38   16   16   16\n",
      "   16   12   12   16   16   12    1    1    1 5463    1    1    1    1\n",
      "   12   12    1    1    1    1    1    1 5463    1    1    1    1 5463\n",
      "    1    1   41   41   41   41 5463   41   41 5463   41   41   36   36\n",
      "   40   40   41   41   14   14    1   12   16   16    1    1 5463    1\n",
      "    1 5463    1    1   12   12   16   16 5463   16   16   36   36   12\n",
      "   12   40   40    1    1 5463    1    1    1    1   12   12   12   12\n",
      " 5463   12   12 5463   12   12    1    1    1    1 5463    1    1    1\n",
      "    1   17   17   17   17 5463   17   17 5463   17   17    1    1   41\n",
      "   41   17   17 5463   17   17    1    1 5463    1    1   14   14   17\n",
      "   17 5463   17   17 5463   17   17 5463   17   17 5463   17   17    1\n",
      "    1 5463    1    1    1    1 5463    1    1   14   14   17   17   17\n",
      "   17 5463   17   17 5463   17   17    1    1 5463    1    1   14   14\n",
      "   17   17 5463   17   17 5463   17   17 5463   17   17    1    1 5463\n",
      "    1    1   14   14   17   17 5463   17   17 5463   17   17 5463   17\n",
      "   17 5463   17   17    1    1 5463    1    1   14   14   36   36   36\n",
      "   36 2628 5463 2628 5463 2628 5463 2628 5463 2628 5463 2628 5463 2628\n",
      " 5463 2628   17   17 5463   17   17    1    1 5463    1    1   14   14\n",
      "   17   17 5463   17   17 5463   17   17 5463   17   17 5463   17   17\n",
      "    1    1  807  807 5463  807  807 5463  807  807 5463  807  807 1147\n",
      " 1147 1147 1147 2211 2211 1285 1285 1285 1285 5463 1285 1285 5463 1285\n",
      " 1285 1285    0  109  109  109  109  109  109  376   37   16   16 5463\n",
      "   16   16   40   40 1094 1094 1218 1218 1094 1094 1094 1094 1117 1117\n",
      "    0    0 1117 1117    0    0 1208 1208    0    0 1829 1829 5463 1829\n",
      " 1829    0    0    0    0 3866 3866 3866 3866 3866 3866 3866 3866   38\n",
      "    0   30    0   42    0   10    0   42    0   10    0   42    0   42\n",
      "    0   42    0   42    0   42    0   42    0   42    0   42    0   42\n",
      "    0   42    0   42    0   42    0   42    0   42    0   42    0   42\n",
      "    0   42    0   42    0   42    0   42    0  212   25  212   25  867\n",
      "  867  867  867  867  867  867  867  867  867  867  867   42    0   42\n",
      "    0   42    0]\n"
     ]
    }
   ],
   "source": [
    "# trainer.load('./experiments/guitar/model-15.pt')\n",
    "# from collections import Counter\n",
    "# start = []\n",
    "# for song in dataset:\n",
    "#     start.append(song[0])\n",
    "# Counter(start).most_common\n",
    "\n",
    "gen = np.array([17] + list(trainer.generate(\"17\", 800)))\n",
    "print (gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_one_hot_bass():\n",
    "    gen_one_hot = []\n",
    "\n",
    "    for i in range(len(gen)):\n",
    "        if gen[i] == 128:\n",
    "            continue\n",
    "        one_hot = np.zeros((128,1))\n",
    "        if gen[i] != 0:\n",
    "            if i == 0 or gen[i] != gen[i-1]:\n",
    "                one_hot[gen[i]] = 1\n",
    "            else:\n",
    "                one_hot[gen[i]] = 0.5\n",
    "        gen_one_hot.append(one_hot)\n",
    "\n",
    "    gen_one_hot = np.hstack(gen_one_hot)\n",
    "    np.save('./try_bass.npy', gen_one_hot)\n",
    "\n",
    "def gen_one_hot_guitar():\n",
    "    gen_one_hot = []\n",
    "    \n",
    "    for i in range(len(gen)):\n",
    "        if gen[i] == len(chord_vocab):\n",
    "            continue\n",
    "        one_hot = np.zeros((128, 1))\n",
    "        chord = chord_vocab[gen[i]]\n",
    "        if i == 0 or gen[i] != gen[i-1]:\n",
    "            for c in chord:\n",
    "                one_hot[c] = 1\n",
    "        else:\n",
    "            for c in chord:\n",
    "                one_hot[c] = 0.5\n",
    "        gen_one_hot.append(one_hot)\n",
    "    gen_one_hot = np.hstack(gen_one_hot)\n",
    "    np.save('./try_guitar.npy', gen_one_hot)\n",
    "\n",
    "gen_one_hot_guitar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
